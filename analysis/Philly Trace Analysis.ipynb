{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGDIR = '../trace-data'\n",
    "DATE_FORMAT_STR = '%Y-%m-%d %H:%M:%S'\n",
    "MINUTES_PER_DAY = (24 * 60)\n",
    "MICROSECONDS_PER_MINUTE = (60 * 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(date_str):\n",
    "    \"\"\"Parses a date string and returns a datetime object if possible.\n",
    "    \n",
    "       Args:\n",
    "           date_str: A string representing a date.\n",
    "        \n",
    "       Returns:\n",
    "           A datetime object if the input string could be successfully\n",
    "           parsed, None otherwise.\n",
    "    \"\"\"\n",
    "    if date_str is None or date_str == '' or date_str == 'None':\n",
    "        return None\n",
    "    return datetime.datetime.strptime(date_str, DATE_FORMAT_STR)\n",
    "\n",
    "def timedelta_to_minutes(timedelta):\n",
    "    \"\"\"Converts a datetime timedelta object to minutes.\n",
    "    \n",
    "       Args:\n",
    "           timedelta: The timedelta to convert.\n",
    "           \n",
    "       Returns:\n",
    "           The number of minutes captured in the timedelta.\n",
    "    \"\"\"\n",
    "    minutes = 0.0\n",
    "    minutes += timedelta.days * MINUTES_PER_DAY\n",
    "    minutes += timedelta.seconds / 60.0\n",
    "    minutes += timedelta.microseconds / MICROSECONDS_PER_MINUTE\n",
    "    return minutes\n",
    "\n",
    "def round_to_nearest_minute(t):\n",
    "    \"\"\"Rounds a datetime object down to the nearest minute.\n",
    "    \n",
    "       Args:\n",
    "           t: A datetime object.\n",
    "           \n",
    "        Returns:\n",
    "            A new rounded down datetime object.\n",
    "    \"\"\"\n",
    "    return t - datetime.timedelta(seconds=t.second, microseconds=t.microsecond)\n",
    "\n",
    "def add_minute(t):\n",
    "    \"\"\"Adds a single minute to a datetime object.\n",
    "    \n",
    "       Args:\n",
    "           t: A datetime object.\n",
    "           \n",
    "        Returns:\n",
    "            A new datetime object with an additional minute.\n",
    "    \"\"\"\n",
    "    return t + datetime.timedelta(seconds=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cdf(data):\n",
    "    \"\"\"Returns the CDF of the given data.\n",
    "    \n",
    "       Args:\n",
    "           data: A list of numerical values.\n",
    "           \n",
    "       Returns:\n",
    "           An pair of lists (x, y) for plotting the CDF.\n",
    "    \"\"\"\n",
    "    sorted_data = sorted(data)\n",
    "    p = 100. * np.arange(len(sorted_data)) / (len(sorted_data) - 1)\n",
    "    return sorted_data, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Job:\n",
    "    \"\"\"Encapsulates a job.\"\"\"\n",
    "    \n",
    "    def __init__(self, status, vc, jobid, attempts, submitted_time, user):\n",
    "        \"\"\"Records job parameters and computes key metrics.\n",
    "        \n",
    "           Stores the passed in arguments as well as the number of GPUs\n",
    "           requested by the job. In addition, computes the queueing delay\n",
    "           as defined as the delta between the submission time and the start\n",
    "           time of the first attempt. Finally, computes run time as defined\n",
    "           as the delta between the initial attempt's start time and the last\n",
    "           attempt's finish time.\n",
    "           \n",
    "           NOTE: Some jobs do not have any recorded attempts, and some attempts\n",
    "           have missing start and/or end times. A job's latest attempt having no\n",
    "           end time indicates that the job was still running when the log data\n",
    "           was collected.\n",
    "   \n",
    "           Args:\n",
    "               status: One of 'Pass', 'Killed', 'Failed'.\n",
    "               vc: The hash of the virtual cluster id the job was run in.\n",
    "               jobid: The hash of the job id.\n",
    "               attempts: A list of dicts, where each dict contains the following keys:\n",
    "                   'start_time': The start time of the attempt.\n",
    "                   'end_time': The end time of the attempt.\n",
    "                   'detail': A list of nested dicts where each dict contains \n",
    "                             the following keys:\n",
    "                        'ip': The server id.\n",
    "                        'gpus': A list of the GPU ids allotted for this attempt.\n",
    "                submitted_time: The time the job was submitted to the queue.\n",
    "                user: The user's id.            \n",
    "        \"\"\"\n",
    "        self._status = status\n",
    "        self._vc = vc\n",
    "        self._jobid = jobid\n",
    "        for attempt in attempts:\n",
    "            attempt['start_time'] = parse_date(attempt['start_time'])\n",
    "            attempt['end_time'] = parse_date(attempt['end_time'])\n",
    "        self._attempts = attempts\n",
    "        self._submitted_time = parse_date(submitted_time)\n",
    "        self._user = user\n",
    "        \n",
    "        if len(self._attempts) == 0:\n",
    "            self._num_gpus = None\n",
    "            self._run_time = None\n",
    "            self._queueing_delay = None\n",
    "        else:\n",
    "            self._num_gpus = sum([len(detail['gpus']) for detail in self._attempts[0]['detail']])\n",
    "            if self._attempts[0]['start_time'] is None:\n",
    "                self._run_time = None\n",
    "                self._queueing_delay = None\n",
    "            else:\n",
    "                if self._attempts[-1]['end_time'] is None:\n",
    "                    self._run_time = None\n",
    "                else:\n",
    "                    self._run_time = \\\n",
    "                        timedelta_to_minutes(self._attempts[-1]['end_time'] -\n",
    "                                             self._attempts[0]['start_time'])\n",
    "                self._queueing_delay = \\\n",
    "                    timedelta_to_minutes(self._attempts[0]['start_time'] -\n",
    "                                         self._submitted_time)\n",
    "    \n",
    "    @property\n",
    "    def status(self):\n",
    "        return self._status\n",
    "    \n",
    "    @property\n",
    "    def vc(self):\n",
    "        return self._vc\n",
    "    \n",
    "    @property\n",
    "    def jobid(self):\n",
    "        return self._jobid\n",
    "    \n",
    "    @property\n",
    "    def attempts(self):\n",
    "        return self._attempts\n",
    "    \n",
    "    @property\n",
    "    def submitted_time(self):\n",
    "        return self._submitted_time\n",
    "    \n",
    "    @property\n",
    "    def user(self):\n",
    "        return self._user\n",
    "    \n",
    "    @property\n",
    "    def num_gpus(self):\n",
    "        return self._num_gpus\n",
    "    \n",
    "    @property\n",
    "    def queueing_delay(self):\n",
    "        return self._queueing_delay\n",
    "    \n",
    "    @property\n",
    "    def run_time(self):\n",
    "        return self._run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bucket_from_num_gpus(num_gpus):\n",
    "    \"\"\"Maps GPU count to a bucket for plotting purposes.\"\"\"\n",
    "    if num_gpus is None:\n",
    "        return None\n",
    "    if num_gpus == 1:\n",
    "        return 0\n",
    "    elif num_gpus >= 2 and num_gpus <= 4:\n",
    "        return 1\n",
    "    elif num_gpus >= 5 and num_gpus <= 8:\n",
    "        return 2\n",
    "    elif num_gpus > 8:\n",
    "        return 3\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def get_plot_config_from_bucket(bucket):\n",
    "    \"\"\"Returns plotting configuration information.\"\"\"\n",
    "    if bucket == 0:\n",
    "        return ('1', 'green', '-')\n",
    "    elif bucket == 1:\n",
    "        return ('2-4', 'blue', '-.')\n",
    "    elif bucket == 2:\n",
    "        return ('5-8', 'red', '--')\n",
    "    elif bucket == 3:\n",
    "        return ('>8', 'purple', ':')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the cluster log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_job_log_path = os.path.join(LOGDIR, 'cluster_job_log')\n",
    "with open(cluster_job_log_path, 'r') as f:\n",
    "    cluster_job_log = json.load(f)\n",
    "jobs = [Job(**job) for job in cluster_job_log]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Runtimes (Figure 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_times = {}\n",
    "for job in jobs:\n",
    "    num_gpus = job.num_gpus\n",
    "    bucket = get_bucket_from_num_gpus(num_gpus)\n",
    "    if bucket is None:\n",
    "        continue\n",
    "    if bucket not in run_times:\n",
    "        run_times[bucket] = []\n",
    "    run_time = job.run_time\n",
    "    if run_time is not None:\n",
    "        run_times[bucket].append(run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = sorted([bucket for bucket in run_times])\n",
    "for bucket in buckets:\n",
    "    num_gpus, color, linestyle = get_plot_config_from_bucket(bucket)\n",
    "    x, y = get_cdf(run_times[bucket])\n",
    "    plt.plot(x, y, label='%s GPU' % (num_gpus), color=color, linestyle=linestyle)\n",
    "plt.legend(loc='lower right')\n",
    "plt.xscale('log')\n",
    "plt.xlim(10 ** -1, 10 ** 4)\n",
    "plt.ylim(0, 100)\n",
    "plt.xlabel('Time (min)')\n",
    "plt.ylabel('CDF')\n",
    "plt.grid(alpha=.3, linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Queueing Delay (Figure 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queueing_delays = {}\n",
    "for job in jobs:\n",
    "    vc = job.vc\n",
    "    if vc not in queueing_delays:\n",
    "        queueing_delays[vc] = {}\n",
    "    bucket = get_bucket_from_num_gpus(job.num_gpus)\n",
    "    if bucket is None:\n",
    "        continue\n",
    "    if bucket not in queueing_delays[vc]:\n",
    "        queueing_delays[vc][bucket] = []\n",
    "    # NOTE: Each period between the job being placed on the queue\n",
    "    # and being scheduled on a machine is recorded as an individual\n",
    "    # queueing delay.\n",
    "    queueing_delay = 0.0\n",
    "    queue_time = job.submitted_time\n",
    "    for attempt in job.attempts:\n",
    "        start_time = attempt['start_time']\n",
    "        if queue_time is not None and start_time is not None:\n",
    "            queueing_delay = timedelta_to_minutes(start_time - queue_time)\n",
    "            queue_time = attempt['end_time']\n",
    "        queueing_delays[vc][bucket].append(queueing_delay)\n",
    "for vc in queueing_delays:\n",
    "    for bucket in queueing_delays[vc]:\n",
    "        queueing_delays[vc][bucket] = filter(None, queueing_delays[vc][bucket])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcs = queueing_delays.keys()\n",
    "for i, vc in enumerate(vcs):\n",
    "    for bucket in queueing_delays[vc]:\n",
    "        num_gpus, color, linestyle = get_plot_config_from_bucket(bucket)\n",
    "        x, y = get_cdf(queueing_delays[vc][bucket])\n",
    "        plt.plot(x, y, label='%s GPU' % (num_gpus), color=color, linestyle=linestyle)\n",
    "    plt.title('VC %s' % (vc))\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xscale('log')\n",
    "    plt.ylim(0, 100)\n",
    "    plt.xlim(10 ** -1, 10 ** 4)\n",
    "    plt.xlabel('Time (min)')\n",
    "    plt.ylabel('CDF')\n",
    "    plt.grid(alpha=.3, linestyle='--')\n",
    "    if i < len(vcs) - 1:\n",
    "        plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locality Constraints (Figure 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for i, job in enumerate(jobs):\n",
    "    if len(job.attempts) == 0:\n",
    "        continue\n",
    "    num_gpus = job.num_gpus\n",
    "    if num_gpus < 5:\n",
    "        continue\n",
    "    bucket = get_bucket_from_num_gpus(num_gpus)\n",
    "    if bucket not in data:\n",
    "        data[bucket] = {\n",
    "            'x': [],\n",
    "            'y': []\n",
    "        }\n",
    "    queueing_delay = job.queueing_delay\n",
    "    num_servers = len(job.attempts[0]['detail'])\n",
    "    data[bucket]['x'].append(queueing_delay)\n",
    "    data[bucket]['y'].append(num_servers)\n",
    "for bucket in data:\n",
    "    num_gpus, _, _ = get_plot_config_from_bucket(bucket)\n",
    "    if bucket == 2:\n",
    "        marker = '+'\n",
    "        facecolors = 'black'\n",
    "        edgecolors = 'none'\n",
    "    else:\n",
    "        marker = 'o'\n",
    "        facecolors = 'none'\n",
    "        edgecolors = 'red'\n",
    "    plt.scatter(data[bucket]['x'], data[bucket]['y'], label='%s GPU' % (num_gpus),\n",
    "                marker=marker, facecolors=facecolors, edgecolors=edgecolors)\n",
    "    plt.legend()\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Time (min)')\n",
    "plt.ylabel('Num. Servers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Utilization (Figures 5, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_util_path = os.path.join(LOGDIR, 'cluster_gpu_util')\n",
    "gpu_util = {}\n",
    "with open(gpu_util_path, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        time = row[0][:-4] # Remove the timezone\n",
    "        machineId = row[1]\n",
    "        if machineId not in gpu_util:\n",
    "            gpu_util[machineId] = {}\n",
    "        gpu_util[machineId][time] = row[2:-1] # Ignore extra empty string at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_utilization_data(jobs, only_large_jobs=False, only_dedicated_servers=False):\n",
    "    \"\"\"Aggregates GPU utilization data for a set of jobs.\n",
    "    \n",
    "       Args:\n",
    "           jobs: A list of Jobs.\n",
    "           only_large_jobs: If True, only considers jobs of size 8 or 16 GPUs.\n",
    "                            Otherwise, considers jobs of size 1, 4, 8, or 16 GPUs.       \n",
    "           only_dedicated_servers: If True, only considers jobs that use all GPUs\n",
    "                                   available on a server(s).\n",
    "       \n",
    "       Returns:\n",
    "           A dict indexed by 1) job completion status, 2) number of GPUs requested\n",
    "           by the job, and 3) timestamp. The value of each nested dict is a list of\n",
    "           percentages indicating the utilization of each individual GPU on the\n",
    "           servers used by the job at the particular time requested.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for job in jobs:\n",
    "        num_gpus = job.num_gpus\n",
    "        if (len(job.attempts) == 0 or\n",
    "            (num_gpus != 1 and num_gpus != 4 and num_gpus != 8 and num_gpus != 16)):\n",
    "            continue\n",
    "        if only_large_jobs and num_gpus < 8:\n",
    "            continue\n",
    "        status = job.status\n",
    "        if status not in data:\n",
    "            data[status] = {}\n",
    "        if num_gpus not in data[status]:\n",
    "            data[status][num_gpus] = []\n",
    "        for attempt in job.attempts:\n",
    "            if only_dedicated_servers and len(attempt['detail']) > (num_gpus / 8):\n",
    "                continue\n",
    "            current_time = attempt['start_time']\n",
    "            if current_time is None or attempt['end_time'] is None:\n",
    "                continue\n",
    "            current_minute = round_to_nearest_minute(current_time)\n",
    "            while current_minute < attempt['end_time']:\n",
    "                current_minute_str = str(current_minute)\n",
    "                for detail in attempt['detail']:\n",
    "                    machineId = detail['ip']\n",
    "                    if current_minute_str in gpu_util[machineId]:\n",
    "                        for gpu_id in detail['gpus']:\n",
    "                            gpu_num = int(gpu_id[3:]) # Remove the 'gpu' prefix\n",
    "                            try:\n",
    "                                u = gpu_util[machineId][current_minute_str][gpu_num]\n",
    "                                if u != 'NA':\n",
    "                                    data[status][num_gpus].append(float(u))\n",
    "                            except Exception as e:\n",
    "                                print(gpu_util[machineId][current_minute_str])\n",
    "                                print(gpu_num)\n",
    "                                raise ValueError(e)\n",
    "                current_minute = add_minute(current_minute)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_utilization_data(jobs)\n",
    "statuses = data.keys()\n",
    "for i, status in enumerate(statuses):\n",
    "    all_num_gpus = sorted(data[status].keys())\n",
    "    for num_gpus in all_num_gpus:\n",
    "        if num_gpus == 1:\n",
    "            color = 'green'\n",
    "            linestyle = '-'\n",
    "        elif num_gpus == 4:\n",
    "            color = 'blue'\n",
    "            linestyle = '-.'\n",
    "        elif num_gpus == 8:\n",
    "            color = 'red'\n",
    "            linestyle = '--'\n",
    "        elif num_gpus == 16:\n",
    "            color = 'cyan'\n",
    "            linestyle = ':'\n",
    "        x, y = get_cdf(data[status][num_gpus])\n",
    "        plt.plot(x, y, label='%s GPU' % (num_gpus), color=color, linestyle=linestyle)\n",
    "    plt.title(status)\n",
    "    plt.xlim(0, 100)\n",
    "    plt.ylim(0, 100)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xlabel('Utilization (%)')\n",
    "    plt.ylabel('CDF')\n",
    "    plt.grid(alpha=.3, linestyle='--')\n",
    "    if i < len(statuses) - 1:\n",
    "        plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_utilization_data(jobs, only_large_jobs=True, only_dedicated_servers=True)\n",
    "aggregate_data = {}\n",
    "for status in data:\n",
    "    for num_gpus in data[status]:\n",
    "        if num_gpus not in aggregate_data:\n",
    "            aggregate_data[num_gpus] = []\n",
    "        aggregate_data[num_gpus] += data[status][num_gpus]\n",
    "all_num_gpus = sorted(aggregate_data.keys())\n",
    "for num_gpus in all_num_gpus:\n",
    "    if num_gpus == 8:\n",
    "        linestyle = '-'\n",
    "    elif num_gpus == 16:\n",
    "        linestyle = '-.'\n",
    "    x, y = get_cdf(aggregate_data[num_gpus])\n",
    "    plt.plot(x, y, label='%s GPU' % (num_gpus), color='black', linestyle=linestyle)\n",
    "plt.xlim(0, 100)\n",
    "plt.ylim(0, 100)\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Utilization (%)')\n",
    "plt.ylabel('CDF')\n",
    "plt.grid(alpha=.3, linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Host Resource Utilization (Figure 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_util_path = os.path.join(LOGDIR, 'cluster_mem_util')\n",
    "mem_util = []\n",
    "with open(mem_util_path, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        if row[2] == 'NA':\n",
    "            continue\n",
    "        mem_total = float(row[2])\n",
    "        mem_free = float(row[3])\n",
    "        if mem_total == 0:\n",
    "            continue\n",
    "        mem_util.append(100.0 * (mem_total - mem_free) / mem_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_util_path = os.path.join(LOGDIR, 'cluster_cpu_util')\n",
    "cpu_util = []\n",
    "with open(cpu_util_path, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        if row[2] == 'NA':\n",
    "            continue\n",
    "        cpu_util.append(float(row[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_cdf(cpu_util)\n",
    "plt.plot(x, y, label='CPU', color='black', linestyle='-')\n",
    "x, y = get_cdf(mem_util)\n",
    "plt.plot(x, y, label='Memory', color='black', linestyle='-.')\n",
    "plt.xlim(0, 100)\n",
    "plt.ylim(0, 100)\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Utilization (%)')\n",
    "plt.ylabel('CDF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Num jobs over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 142, 23, 24, 25, 26, 147, 30, 31, 32, 33, 35, 37, 39, 40, 41, 43, 44, 48, 50, 55, 56, 57, 59, 60, 61, 65, 70, 141, 81, 85, 88, 89, 90, 91, 92, 93, 95, 96, 100, 102, 111}\n"
     ]
    }
   ],
   "source": [
    "jobs[0].attempts[0]['detail'][0]['ip']\n",
    "num_attempts = []\n",
    "for i in range(len(jobs)):\n",
    "    num_attempts.append(len(jobs[i].attempts))\n",
    "print(set(num_attempts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start_time': datetime.datetime(2017, 10, 9, 7, 2, 5),\n",
       " 'end_time': datetime.datetime(2017, 10, 9, 7, 3, 11),\n",
       " 'detail': [{'ip': 'm372', 'gpus': ['gpu0']}]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs[0].attempts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = [job for job in jobs if len(job.attempts) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "attempts = [attempt for job in jobs for attempt in job.attempts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "mid_to_attempts = OrderedDict()\n",
    "for attempt in attempts:\n",
    "    for detail in attempt['detail']:\n",
    "        if detail['ip'] in mid_to_attempts:\n",
    "            mid_to_attempts[detail['ip']].append((attempt['start_time'], attempt['end_time']))\n",
    "        else:\n",
    "            mid_to_attempts[detail['ip']] = [(attempt['start_time'], attempt['end_time'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "total_attempts = [len(v) for v in mid_to_attempts.values()]\n",
    "makespans = [max([end for start, end in values if end is not None]) - min([start for start, end in values if start is not None])\n",
    " for values in mid_to_attempts.values()]\n",
    "attempt_lengths = [statistics.mean([(end - start).total_seconds() for start, end in values if start is not None and end is not None]) \n",
    "                   for values in mid_to_attempts.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3280\n",
      "10022.0 9605569.0\n",
      "963.6666666666666 171700.93103448275\n"
     ]
    }
   ],
   "source": [
    "print(min(total_attempts), max(total_attempts))\n",
    "print(min(makespans).total_seconds(), max(makespans).total_seconds())\n",
    "print(min(attempt_lengths), max(attempt_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "attempt_traces = [sorted([(start, 1) for start, end in values if start is not None and end is not None] + \n",
    " [(end, -1) for start, end in values if start is not None and end is not None]) \n",
    " for values in mid_to_attempts.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_overlaps = []\n",
    "for trace in attempt_traces:\n",
    "    overlaps = []\n",
    "    cur_attempts = 0\n",
    "    for event in trace:\n",
    "        cur_attempts += event[1]\n",
    "        overlaps.append(cur_attempts)\n",
    "    max_overlaps.append(max(overlaps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4, 5, 6, 7, 8, 9}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(max_overlaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_list = os.path.join(LOGDIR, 'cluster_machine_list')\n",
    "machines = {}\n",
    "with open(machine_list, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        machines[row[0]] = int(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.125, 0.25, 0.375, 0.5, 0.625, 1.0, 2.5, 3.0, 3.5, 4.0, 4.5}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([jobs / gpus for jobs, gpus in zip(max_overlaps, [machines[k] for k in mid_to_attempts.keys()])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
